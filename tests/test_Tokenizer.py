# -*- coding: UTF-8 -*-

import sys
from os.path import dirname
sys.path.append(dirname(dirname(sys.path[0])))

from usfmparser.Tokenizer import Tokenizer, TokensDeque
from usfmparser.Token import Token

D = TokensDeque


#### PARAGRAPH

# Don't tokenize '\\h' as TYPE_TEXT
t = Tokenizer()
t.add('\\')
t.tokenize()
assert(t.tokens == D([]))
t.add('h')
t.tokenize()
assert(t.tokens == D([]))

# ... tokenize it as '\\h ', not until a space appears
t.add(' ')
t.tokenize()
assert(t.tokens == D([Token(Token.TYPE_MARKER_START, '\\h ', 0, 3)]))


t = Tokenizer()
t.add('\\h Genesis\n\\')
t.tokenize()
assert(t.tokens == D([
    Token(Token.TYPE_MARKER_START, '\\h ', 0, 3),
    Token(Token.TYPE_TEXT, 'Genesis', 3, 10),
    Token(Token.TYPE_NEWLINE, '\n', 10, 11)
]))
t.add('c\t\t1 ')
t.tokenize()
# TODO - keep tokens after provided to consumer? 
assert(t.tokens == D([
    Token(Token.TYPE_MARKER_START, '\\h ', 0, 3),
    Token(Token.TYPE_TEXT, 'Genesis', 3, 10),
    Token(Token.TYPE_NEWLINE, '\n', 10, 11),
    Token(Token.TYPE_MARKER_START, '\\c\t', 11, 14),
    Token(Token.TYPE_SPACE, '\t', 14, 15),
    Token(Token.TYPE_TEXT, '1', 15, 16),
    Token(Token.TYPE_SPACE, ' ', 16, 17)
]))


###### ATTRIBUTES (tokenizing)

t = Tokenizer('something|else')
t.tokenize()
assert(t.tokens == D([
    Token(Token.TYPE_TEXT, value='something', start=0, end=9),
    Token(Token.TYPE_ATTRIBUTE, value='|else', start=9, end=14)
]))

t = Tokenizer('something|lemma=grace')
t.tokenize()
assert(t.tokens == D([
    Token(Token.TYPE_TEXT, value='something', start=0, end=9),
    Token(Token.TYPE_ATTRIBUTE, value='|lemma=grace', start=9, end=21)
]))

t = Tokenizer('something|lemma="grace"')
t.tokenize()
assert(t.tokens == D([
    Token(Token.TYPE_TEXT, value='something', start=0, end=9),
    Token(Token.TYPE_ATTRIBUTE, value='|lemma="grace"', start=9, end=23)
]))

t = Tokenizer('something|lemma="grace, peace"')
t.tokenize()
assert(t.tokens == D([
    Token(Token.TYPE_TEXT, value='something', start=0, end=9),
    Token(Token.TYPE_ATTRIBUTE, value='|lemma="grace, peace"', start=9, end=30)
]))

t = Tokenizer('something|lemma="grace, peace" strong="H1234"')
t.tokenize()
assert(t.tokens == D([
    Token(Token.TYPE_TEXT, value='something', start=0, end=9),
    Token(Token.TYPE_ATTRIBUTE, value='|lemma="grace, peace" strong="H1234"', start=9, end=45)
]))




#### CHARACTER / NOTE




t = Tokenizer('\\wj \\+w foo \\wj*')
t.tokenize()
assert(t.tokens == D([
    Token(Token.TYPE_MARKER_START, '\\wj ', start=0, end=4),
    Token(Token.TYPE_MARKER_START, '\\+w ', start=4, end=8),
    Token(Token.TYPE_TEXT, 'foo', start=8, end=11),
    Token(Token.TYPE_SPACE, ' ', start=11, end=12),
    Token(Token.TYPE_MARKER_END, '\\wj*', start=12, end=16),
]))



#### Milestones



t = Tokenizer('\\qt-s\\*')
t.tokenize()
assert(t.tokens)
assert(len(t.tokens) == 1)
assert(t.tokens[0] == Token(Token.TYPE_MARKER_END, '\\qt-s\\*', start=0, end=7))

# e.g. \\qt-s* (missing \\) should be recognized in parser

t = Tokenizer('\\wj-s\\* \\wj \\w something|strong="G5678" \\w*\\w yo|lemma="yo"\\w*\\wj-e\\*')
t.tokenize()
assert(t.tokens)




assert(len(t.tokens) == 12)
assert(t.tokens[0] == Token(Token.TYPE_MARKER_END, '\\wj-s\\*', start=0, end=7))
assert(t.tokens[1] == Token(Token.TYPE_SPACE, ' ', start=7, end=8))
assert(t.tokens[2] == Token(Token.TYPE_MARKER_START, '\\wj ', start=8, end=12))
assert(t.tokens[3] == Token(Token.TYPE_MARKER_START, '\\w ', start=12, end=15))
assert(t.tokens[4] == Token(Token.TYPE_TEXT, 'something', start=15, end=24))
assert(t.tokens[5] == Token(Token.TYPE_ATTRIBUTE, '|strong="G5678" ', start=24, end=40))
assert(t.tokens[6] == Token(Token.TYPE_MARKER_END, '\\w*', start=40, end=43))
assert(t.tokens[7] == Token(Token.TYPE_MARKER_START, '\\w ', start=43, end=46))

assert(t.tokens[8] == Token(Token.TYPE_TEXT, value='yo', start=46, end=48))
assert(t.tokens[9] == Token(Token.TYPE_ATTRIBUTE, '|lemma="yo"', start=48, end=59))
assert(t.tokens[10] == Token(Token.TYPE_MARKER_END, '\\w*', start=59, end=62))


assert(t.tokens[11] == Token(Token.TYPE_MARKER_END, '\\wj-e\\*', start=62, end=69))

t = Tokenizer('\\foo\\*')
t.tokenize()
assert(t.tokens)
assert(len(t.tokens) == 1)
assert(t.tokens[0] == Token(Token.TYPE_MARKER_END, '\\foo\\*', start=0, end=6))


#### Some big chunks of text

t = Tokenizer()
t.add('''
        \\id SNG World English Bible (WEB)
    \\ide UTF-8
    \\h Song of Solomon
    \\toc1 The Song of Solomon
    \\toc2 Song of Solomon
    \\toc3 Song of Solomon
    \\mt1 The Song of Solomon
    \\c 1
    \\p

''')

t.tokenize()
t.tokens.reverse()

assert(t.tokens.pop() == Token(Token.TYPE_NEWLINE, value='\n', start=0, end=1))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=1, end=2))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=2, end=3))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=3, end=4))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=4, end=5))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=5, end=6))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=6, end=7))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=7, end=8))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=8, end=9))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\id ', start=9, end=13))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='SNG', start=13, end=16))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=16, end=17))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='World', start=17, end=22))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=22, end=23))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='English', start=23, end=30))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=30, end=31))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='Bible', start=31, end=36))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=36, end=37))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='(WEB)', start=37, end=42))
assert(t.tokens.pop() == Token(Token.TYPE_NEWLINE, value='\n', start=42, end=43))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=43, end=44))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=44, end=45))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=45, end=46))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=46, end=47))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\ide ', start=47, end=52))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='UTF-8', start=52, end=57))
assert(t.tokens.pop() == Token(Token.TYPE_NEWLINE, value='\n', start=57, end=58))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=58, end=59))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=59, end=60))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=60, end=61))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=61, end=62))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\h ', start=62, end=65))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='Song', start=65, end=69))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=69, end=70))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='of', start=70, end=72))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=72, end=73))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='Solomon', start=73, end=80))
assert(t.tokens.pop() == Token(Token.TYPE_NEWLINE, value='\n', start=80, end=81))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=81, end=82))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=82, end=83))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=83, end=84))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=84, end=85))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\toc1 ', start=85, end=91))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='The', start=91, end=94))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=94, end=95))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='Song', start=95, end=99))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=99, end=100))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='of', start=100, end=102))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=102, end=103))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='Solomon', start=103, end=110))
assert(t.tokens.pop() == Token(Token.TYPE_NEWLINE, value='\n', start=110, end=111))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=111, end=112))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=112, end=113))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=113, end=114))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=114, end=115))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\toc2 ', start=115, end=121))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='Song', start=121, end=125))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=125, end=126))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='of', start=126, end=128))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=128, end=129))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='Solomon', start=129, end=136))
assert(t.tokens.pop() == Token(Token.TYPE_NEWLINE, value='\n', start=136, end=137))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=137, end=138))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=138, end=139))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=139, end=140))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=140, end=141))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\toc3 ', start=141, end=147))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='Song', start=147, end=151))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=151, end=152))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='of', start=152, end=154))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=154, end=155))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='Solomon', start=155, end=162))
assert(t.tokens.pop() == Token(Token.TYPE_NEWLINE, value='\n', start=162, end=163))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=163, end=164))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=164, end=165))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=165, end=166))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=166, end=167))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\mt1 ', start=167, end=172))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='The', start=172, end=175))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=175, end=176))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='Song', start=176, end=180))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=180, end=181))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='of', start=181, end=183))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=183, end=184))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='Solomon', start=184, end=191))
assert(t.tokens.pop() == Token(Token.TYPE_NEWLINE, value='\n', start=191, end=192))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=192, end=193))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=193, end=194))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=194, end=195))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=195, end=196))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\c ', start=196, end=199))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='1', start=199, end=200))
assert(t.tokens.pop() == Token(Token.TYPE_NEWLINE, value='\n', start=200, end=201))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=201, end=202))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=202, end=203))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=203, end=204))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=204, end=205))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\p\n', start=205, end=208))
assert(t.tokens.pop() == Token(Token.TYPE_NEWLINE, value='\n', start=208, end=209))


t = Tokenizer('''
\\q1
\\v 15  \\w Behold|strong="H2005"\\w*,\\f + \\fr 1:15  \\ft “Behold”, from “הִנֵּה”, means look at, take notice, observe, see, or gaze at. It is often used as an interjection.\\f* \\w you|strong="H5869"\\w* \\w are|strong="H5869"\\w* \\w beautiful|strong="H3303"\\w*, my love.
\\q2 \\w Behold|strong="H2005"\\w*, \\w you|strong="H5869"\\w* \\w are|strong="H5869"\\w* \\w beautiful|strong="H3303"\\w*.
\\q2 \\w Your|strong="H3303"\\w* \\w eyes|strong="H5869"\\w* \\w are|strong="H5869"\\w* \\w like|strong="H5869"\\w* \\w doves|strong="H3123"\\w*.
\\sp Beloved
\\q1
\\v 16  \\w Behold|strong="H2005"\\w*, \\w you|strong="H0637"\\w* \\w are|strong="H0637"\\w* \\w beautiful|strong="H3303"\\w*, \\w my|strong="H0637"\\w* \\w beloved|strong="H1730"\\w*, \\w yes|strong="H0637"\\w*, \\w pleasant|strong="H5273"\\w*;
\\q2 \\w and|strong="H3303"\\w* our \\w couch|strong="H6210"\\w* \\w is|strong="H1730"\\w* verdant.
\\sp Lover
\\q1
\\v 17  \\w The|strong="H1004"\\w* \\w beams|strong="H6982"\\w* \\w of|strong="H1004"\\w* our \\w house|strong="H1004"\\w* \\w are|strong="H1004"\\w* \\w cedars|strong="H0730"\\w*.
\\q2 Our \\w rafters|strong="H7351"\\w* \\w are|strong="H1004"\\w* firs.
\\c 2
\\sp Beloved
\\q1
\\v 1  \\w I|strong="H0589"\\w* \\w am|strong="H0589"\\w* a \\w rose|strong="H2261"\\w* \\w of|strong="H6010"\\w* \\w Sharon|strong="H8289"\\w*,
\\q2 a \\w lily|strong="H7799"\\w* \\w of|strong="H6010"\\w* \\w the|strong="H0589"\\w* \\w valleys|strong="H6010"\\w*.
\\sp Lover
\\q1
''')

t.tokenize()
t.tokens.reverse()

assert(t.tokens.pop() == Token(Token.TYPE_NEWLINE, value='\n', start=0, end=1))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\q1\n', start=1, end=5))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\v ', start=5, end=8))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='15', start=8, end=10))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=10, end=11))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=11, end=12))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=12, end=15))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='Behold', start=15, end=21))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H2005"', start=21, end=36))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=36, end=39))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value=',', start=39, end=40))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\f ', start=40, end=43))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='+', start=43, end=44))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=44, end=45))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\fr ', start=45, end=49))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='1:15', start=49, end=53))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=53, end=54))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=54, end=55))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\ft ', start=55, end=59))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='“Behold”,', start=59, end=68))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=68, end=69))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='from', start=69, end=73))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=73, end=74))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='“הִנֵּה”,', start=74, end=83))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=83, end=84))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='means', start=84, end=89))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=89, end=90))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='look', start=90, end=94))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=94, end=95))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='at,', start=95, end=98))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=98, end=99))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='take', start=99, end=103))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=103, end=104))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='notice,', start=104, end=111))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=111, end=112))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='observe,', start=112, end=120))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=120, end=121))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='see,', start=121, end=125))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=125, end=126))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='or', start=126, end=128))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=128, end=129))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='gaze', start=129, end=133))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=133, end=134))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='at.', start=134, end=137))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=137, end=138))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='It', start=138, end=140))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=140, end=141))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='is', start=141, end=143))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=143, end=144))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='often', start=144, end=149))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=149, end=150))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='used', start=150, end=154))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=154, end=155))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='as', start=155, end=157))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=157, end=158))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='an', start=158, end=160))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=160, end=161))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='interjection.', start=161, end=174))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\f*', start=174, end=177))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=177, end=178))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=178, end=181))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='you', start=181, end=184))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H5869"', start=184, end=199))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=199, end=202))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=202, end=203))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=203, end=206))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='are', start=206, end=209))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H5869"', start=209, end=224))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=224, end=227))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=227, end=228))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=228, end=231))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='beautiful', start=231, end=240))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H3303"', start=240, end=255))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=255, end=258))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value=',', start=258, end=259))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=259, end=260))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='my', start=260, end=262))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=262, end=263))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='love.', start=263, end=268))
assert(t.tokens.pop() == Token(Token.TYPE_NEWLINE, value='\n', start=268, end=269))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\q2 ', start=269, end=273))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=273, end=276))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='Behold', start=276, end=282))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H2005"', start=282, end=297))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=297, end=300))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value=',', start=300, end=301))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=301, end=302))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=302, end=305))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='you', start=305, end=308))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H5869"', start=308, end=323))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=323, end=326))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=326, end=327))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=327, end=330))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='are', start=330, end=333))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H5869"', start=333, end=348))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=348, end=351))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=351, end=352))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=352, end=355))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='beautiful', start=355, end=364))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H3303"', start=364, end=379))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=379, end=382))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='.', start=382, end=383))
assert(t.tokens.pop() == Token(Token.TYPE_NEWLINE, value='\n', start=383, end=384))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\q2 ', start=384, end=388))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=388, end=391))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='Your', start=391, end=395))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H3303"', start=395, end=410))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=410, end=413))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=413, end=414))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=414, end=417))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='eyes', start=417, end=421))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H5869"', start=421, end=436))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=436, end=439))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=439, end=440))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=440, end=443))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='are', start=443, end=446))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H5869"', start=446, end=461))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=461, end=464))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=464, end=465))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=465, end=468))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='like', start=468, end=472))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H5869"', start=472, end=487))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=487, end=490))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=490, end=491))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=491, end=494))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='doves', start=494, end=499))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H3123"', start=499, end=514))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=514, end=517))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='.', start=517, end=518))
assert(t.tokens.pop() == Token(Token.TYPE_NEWLINE, value='\n', start=518, end=519))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\sp ', start=519, end=523))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='Beloved', start=523, end=530))
assert(t.tokens.pop() == Token(Token.TYPE_NEWLINE, value='\n', start=530, end=531))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\q1\n', start=531, end=535))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\v ', start=535, end=538))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='16', start=538, end=540))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=540, end=541))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=541, end=542))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=542, end=545))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='Behold', start=545, end=551))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H2005"', start=551, end=566))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=566, end=569))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value=',', start=569, end=570))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=570, end=571))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=571, end=574))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='you', start=574, end=577))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H0637"', start=577, end=592))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=592, end=595))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=595, end=596))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=596, end=599))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='are', start=599, end=602))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H0637"', start=602, end=617))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=617, end=620))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=620, end=621))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=621, end=624))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='beautiful', start=624, end=633))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H3303"', start=633, end=648))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=648, end=651))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value=',', start=651, end=652))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=652, end=653))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=653, end=656))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='my', start=656, end=658))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H0637"', start=658, end=673))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=673, end=676))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=676, end=677))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=677, end=680))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='beloved', start=680, end=687))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H1730"', start=687, end=702))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=702, end=705))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value=',', start=705, end=706))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=706, end=707))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=707, end=710))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='yes', start=710, end=713))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H0637"', start=713, end=728))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=728, end=731))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value=',', start=731, end=732))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=732, end=733))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=733, end=736))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='pleasant', start=736, end=744))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H5273"', start=744, end=759))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=759, end=762))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value=';', start=762, end=763))
assert(t.tokens.pop() == Token(Token.TYPE_NEWLINE, value='\n', start=763, end=764))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\q2 ', start=764, end=768))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=768, end=771))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='and', start=771, end=774))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H3303"', start=774, end=789))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=789, end=792))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=792, end=793))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='our', start=793, end=796))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=796, end=797))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=797, end=800))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='couch', start=800, end=805))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H6210"', start=805, end=820))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=820, end=823))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=823, end=824))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=824, end=827))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='is', start=827, end=829))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H1730"', start=829, end=844))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=844, end=847))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=847, end=848))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='verdant.', start=848, end=856))
assert(t.tokens.pop() == Token(Token.TYPE_NEWLINE, value='\n', start=856, end=857))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\sp ', start=857, end=861))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='Lover', start=861, end=866))
assert(t.tokens.pop() == Token(Token.TYPE_NEWLINE, value='\n', start=866, end=867))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\q1\n', start=867, end=871))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\v ', start=871, end=874))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='17', start=874, end=876))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=876, end=877))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=877, end=878))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=878, end=881))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='The', start=881, end=884))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H1004"', start=884, end=899))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=899, end=902))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=902, end=903))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=903, end=906))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='beams', start=906, end=911))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H6982"', start=911, end=926))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=926, end=929))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=929, end=930))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=930, end=933))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='of', start=933, end=935))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H1004"', start=935, end=950))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=950, end=953))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=953, end=954))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='our', start=954, end=957))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=957, end=958))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=958, end=961))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='house', start=961, end=966))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H1004"', start=966, end=981))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=981, end=984))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=984, end=985))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=985, end=988))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='are', start=988, end=991))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H1004"', start=991, end=1006))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=1006, end=1009))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=1009, end=1010))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=1010, end=1013))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='cedars', start=1013, end=1019))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H0730"', start=1019, end=1034))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=1034, end=1037))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='.', start=1037, end=1038))
assert(t.tokens.pop() == Token(Token.TYPE_NEWLINE, value='\n', start=1038, end=1039))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\q2 ', start=1039, end=1043))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='Our', start=1043, end=1046))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=1046, end=1047))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=1047, end=1050))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='rafters', start=1050, end=1057))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H7351"', start=1057, end=1072))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=1072, end=1075))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=1075, end=1076))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=1076, end=1079))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='are', start=1079, end=1082))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H1004"', start=1082, end=1097))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=1097, end=1100))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=1100, end=1101))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='firs.', start=1101, end=1106))
assert(t.tokens.pop() == Token(Token.TYPE_NEWLINE, value='\n', start=1106, end=1107))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\c ', start=1107, end=1110))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='2', start=1110, end=1111))
assert(t.tokens.pop() == Token(Token.TYPE_NEWLINE, value='\n', start=1111, end=1112))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\sp ', start=1112, end=1116))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='Beloved', start=1116, end=1123))
assert(t.tokens.pop() == Token(Token.TYPE_NEWLINE, value='\n', start=1123, end=1124))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\q1\n', start=1124, end=1128))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\v ', start=1128, end=1131))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='1', start=1131, end=1132))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=1132, end=1133))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=1133, end=1134))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=1134, end=1137))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='I', start=1137, end=1138))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H0589"', start=1138, end=1153))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=1153, end=1156))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=1156, end=1157))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=1157, end=1160))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='am', start=1160, end=1162))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H0589"', start=1162, end=1177))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=1177, end=1180))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=1180, end=1181))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='a', start=1181, end=1182))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=1182, end=1183))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=1183, end=1186))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='rose', start=1186, end=1190))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H2261"', start=1190, end=1205))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=1205, end=1208))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=1208, end=1209))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=1209, end=1212))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='of', start=1212, end=1214))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H6010"', start=1214, end=1229))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=1229, end=1232))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=1232, end=1233))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=1233, end=1236))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='Sharon', start=1236, end=1242))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H8289"', start=1242, end=1257))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=1257, end=1260))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value=',', start=1260, end=1261))
assert(t.tokens.pop() == Token(Token.TYPE_NEWLINE, value='\n', start=1261, end=1262))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\q2 ', start=1262, end=1266))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='a', start=1266, end=1267))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=1267, end=1268))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=1268, end=1271))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='lily', start=1271, end=1275))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H7799"', start=1275, end=1290))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=1290, end=1293))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=1293, end=1294))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=1294, end=1297))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='of', start=1297, end=1299))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H6010"', start=1299, end=1314))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=1314, end=1317))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=1317, end=1318))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=1318, end=1321))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='the', start=1321, end=1324))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H0589"', start=1324, end=1339))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=1339, end=1342))
assert(t.tokens.pop() == Token(Token.TYPE_SPACE, value=' ', start=1342, end=1343))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\w ', start=1343, end=1346))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='valleys', start=1346, end=1353))
assert(t.tokens.pop() == Token(Token.TYPE_ATTRIBUTE, value='|strong="H6010"', start=1353, end=1368))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_END, value='\\w*', start=1368, end=1371))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='.', start=1371, end=1372))
assert(t.tokens.pop() == Token(Token.TYPE_NEWLINE, value='\n', start=1372, end=1373))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\sp ', start=1373, end=1377))
assert(t.tokens.pop() == Token(Token.TYPE_TEXT, value='Lover', start=1377, end=1382))
assert(t.tokens.pop() == Token(Token.TYPE_NEWLINE, value='\n', start=1382, end=1383))
assert(t.tokens.pop() == Token(Token.TYPE_MARKER_START, value='\\q1\n', start=1383, end=1387))



